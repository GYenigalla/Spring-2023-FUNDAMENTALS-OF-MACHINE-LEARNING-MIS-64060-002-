---
title: "fml assignment 5"
author: "Gayathri Yenigalla"
date: "2023-04-16"
output:
  html_document: default
  word_document: default
  pdf_document: default
---
#Importing dataset
```{r}
CEREALS <- read.csv("C:/Users/gaya3/Downloads/CEREALS.csv")
head(CEREALS)
dim(CEREALS)
```

#Loading packages
```{r}
library(cluster)
library(dplyr)
library(caret)
library(dendextend)
library(knitr)
library(factoextra)
library(readr)
```
#Omitting the NUll values
```{r}
CEREALS <- na.omit(CEREALS)
dim(CEREALS)
head(CEREALS)
```
#Creating a dataset with the Numeric Values
```{r}
df1<- data.frame(CEREALS[,4:16])
df2 <- na.omit(df1)
```
#Normalizing the data
```{r}
df1 <- scale(df1)
head(df1)
```
#Applying hierarchical clustering using Euclidean distanceance method.
```{r}
distance <- dist(df1, method= "euclidean")
Hist_cluster <- hclust(distance, method = "complete")
```
#Plotting of the dendogram
```{r}
plot(Hist_cluster, cex = 0.7, hang = -1)
```
#Using Agnes function to perform clustering with single linkage, complete linkage average linkage and Ward.
```{r}
hc.single <- agnes(df1, method = "single")
hc.complete <- agnes(df1, method = "complete")
hc.average <- agnes(df1, method = "average")
hc.ward <- agnes(df1, method ="ward")
```
#Determining the best method
```{r}
print(hc.single$ac)
print(hc.complete$ac)
print(hc.average$ac)
print(hc.ward$ac)
```
#With a rating of 0.9046042, the ward technique is superior to the other methods.

#Choosing the number of clusters
```{r}
pltree(hc.ward, cex = 0.6, hang = -1, main = "Dendrogram of agnes") 
df2_5 <-cutree(hc.ward, k = 5)
rect.hclust(hc.ward , k=5, border = 2:7)
subGroup <- cutree(hc.ward, k=5)

df2_5 <- as.data.frame(cbind(df2_5,subGroup))
fviz_cluster(list(data=df2_5, cluster = subGroup))
```
#It is concluded that 5 clusters can be selected.

#Creating Partitions
```{r}
set.seed(123)
df_A <-df2 [1:55,]
df_B <-df2 [56:74,]
```

#Performing Hierarchial Clustering,considering k = 5.
```{r}
Ag.single <- agnes(scale(df_A), method = "single")
Ag.complete <- agnes(scale(df_A), method = "complete")
Ag.average <- agnes(scale(df_A), method = "average")
Ag.ward <- agnes(scale(df_A), method = "ward")


cbind(single= Ag.single$ac , complete=Ag.complete$ac , average= Ag.average$ac , ward= Ag.ward$ac)

pltree(Ag.ward, cex = 0.6, hang = -1, main = "Dendogram of Agnes Using Ward")
rect.hclust(Ag.ward, k = 5, border = 2:7)
cut2 <- cutree(Ag.ward, k = 5)
```
#Calculating the centroids.
```{r}
Result <- as.data.frame(cbind(df_A, cut2))
Result[Result$cut2==1,]

Centroid1 <- colMeans(Result[Result$cut2==1,])
Result[Result$cut2==2,]

Centroid2 <- colMeans(Result[Result$cut2==2,])
Result[Result$cut2==3,]

Centroid3 <- colMeans(Result[Result$cut2==3,])
Result[Result$cut2==4,]

Centroid4 <- colMeans(Result[Result$cut2==4,])

Centroids <- rbind(Centroid1, Centroid2, Centroid3, Centroid4)
x2 <- as.data.frame(rbind(Centroids[,-14], df_B))
```
#Calculating the distanceance.
```{r}
#df_A_distance
#Clustered_df_A <-cutree ()
#Clusters_A <-as.data.frame(cbind(df_A, Clustered_df_A))
#Identify clusters in each partition.
#Clust.1 <-colMeans (Clusters_A [Clusters_A$ Clustered_df_A == “1” ,])
#Centroid <-rbind(Clust.1, Clust.2, ……)



distance1 <- get_dist(x2)
Matrix <- as.matrix(distance1)
data.frame <- data.frame(data=seq(1,nrow(df_B),1), Clusters = rep(0,nrow(df_B)))

for(i in 1:nrow(df_B)) 
{data.frame[i,2] <- which.min(Matrix[i+4, 1:4])}
data.frame

cbind(df2$SubGroup[51:74], data.frame$Clusters)
table(df2$SubGroup[51:74] == data.frame$Clusters)

#We can conclude that it is partially stable.

```
#Clustering Healthy CEREALS.
```{r}
Healthy_CEREALS <- CEREALS
Healthy_CEREALS_na <- na.omit(Healthy_CEREALS)
Clust_healthy <- cbind(Healthy_CEREALS_na, subGroup)
                  
Clust_healthy[Clust_healthy$subGroup==1,]
Clust_healthy[Clust_healthy$subGroup==2,]
Clust_healthy[Clust_healthy$subGroup==3,]
Clust_healthy[Clust_healthy$subGroup==4,]

```
#Mean ratings to determine the best cluster.
```{r}
mean(Clust_healthy[Clust_healthy$subGroup==1,"rating"])
mean(Clust_healthy[Clust_healthy$subGroup==2,"rating"])
mean(Clust_healthy[Clust_healthy$subGroup==3,"rating"])
mean(Clust_healthy[Clust_healthy$subGroup==4,"rating"])

#We can conclude that cluster 1 should be picked since it has the highest value. Cluster 1 can therefore be regarded as a Healthy Cluster.
```
















